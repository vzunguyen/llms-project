# Project Brief
This project is based on the workshop [Building LLMs from the Ground Up: A 3-hour Coding Workshop](https://magazine.sebastianraschka.com/p/building-llms-from-the-ground-up). It is my first exploration into building a language model from scratch.

## Project Overview
The goal of this project is to understand the inner workings of large language models (LLMs) by implementing one from the ground up.

## Project Progress
- Data Processing
- Data Set, Data Load
- GPT architecture
    - Multihead Attention
    - Layer Norm
    - GELU
    - Feed Forward
    - Transformer Block
- Generating new text (Untrained)
- Weightloading, train model

## Technology being used in LLM Project

## How to run?

# References
[Building LLMs from the Ground Up: A 3-hour Coding Workshop](https://magazine.sebastianraschka.com/p/building-llms-from-the-ground-up)

Apart from "Building LLMs from the Ground Up: A 3-hour Coding Workshop", [Create a Large Language Model from Scratch with Python â€“ Tutorial](https://www.youtube.com/watch?v=UU1WVnMk4E8) was also used to help with developing the project.

***This project is developed by Zu***